
------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------
-- Programming Pig - Exercises

-- Pig Utilities
-- Pig Basics
-- Pig Complex Data Types
-- Pig Functions
--    Datetime function
--    String function
--    Conditional function
-- Pig Relational Operations
--    Filter, group, cogroup
--    Join, flatten, nested foreach, Cross
-- Working with Pig UDFs
--    Piggybank, Pigeon, DataFu

------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------
Dataset
--------------------------------------
-- 1. In this lab session, we will start working with Pig Shell - Grunt
-- 2. File full_text.txt should be available in /home/user_name/lab
-- 3. We will also use dayofweek.txt
-- 5. Lines that begin with "--" are comments
-- 6. Lines that begin with "[hdfs@sandbox ~]" are Hadoop or Linux commands 
-- 7. Lines with no prefix are Pig commands.

------------------------------------------------------------------------------------------------------------------



-------------------------------
-- Lab Data Preparation
-------------------------------


-- Run the following shell commands to place full_text.txt under /user/pig folder in HDFS

-- 1. create a working directory in Linux and hdfs for this exercise

[hdfs@sandbox ~]$ hadoop fs -mkdir /user/pig
[hdfs@sandbox ~]$ cd /home/YourUsername/lab
[hdfs@sandbox lab]$ ll

-- 2. go to the directory that stores full_text.txt file in Linux and load it into HDFS (Same method as we did previously)

[hdfs@sandbox lab]$ hadoop fs -put full_text.txt /user/pig/
[hdfs@sandbox lab]$ hadoop fs -cat /user/pig/full_text.txt | head 



----------------------------------------------------------------------------
-- 2. Pig Latin Basics
----------------------------------------------------------------------------

-- 2.1 Data exploration using limit and dump

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = limit a 5;
dump b;

-- 2.2 load and store data

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
rmf /user/pig/full_text_1.txt
c = store b into '/user/pig/full_text_1.txt';

-- 2.3 Referencing fields (using position and field names)

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate $0, $1, location, tweet;
c = limit b 5;
dump c;

describe b;

----------------------------------------------------------------------------
-- 3. Complex Data Types
----------------------------------------------------------------------------

-- 3.1 MAP example 1

quit

[hdfs@sandbox ~]$ echo -e "user1\t{([address#2436 mains st]),([name#anil]),([phone#222-222-2222]),([city#toronto])} \nuser2\t{([address#456 king st]),([name#jenny]),([occupation#doctor]),([city#toronto])}\nuser3\t{([city#mississauga]),([name#larry]),([interest#sports])}" > data_test_map

[hdfs@sandbox ~]$ cat data_test_map
[hdfs@sandbox ~]$ hadoop fs -put data_test_map '/user/pig/data_test_map'
[hdfs@sandbox ~]$ pig

a = load '/user/pig/data_test_map' using PigStorage('\t') as (id:chararray, info:bag{t:(m:map[])});
b = foreach a generate id, info, flatten(info) as info_flat;
c = filter b by info_flat#'city'=='toronto';
d = limit c 5;
dump d;



-- 3.2 MAP example 2
-- data prep (transformations on the original full_text file and store into another file in HDFS)

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, TOTUPLE(lat, lon) as loc_tuple:tuple(lat:chararray, lon:chararray), flatten(TOKENIZE(tweet)) as token;
c = group b by (id, token);
d = foreach c generate flatten(group) as (id, token), COUNT(b) as cnt; 
e = group d by id;
f = foreach e generate group as id, flatten(TOP(10, 2, d)) as (id1, word,cnt);
g = foreach f generate id, TOMAP(word, cnt) as freq_word:map[];
h = group g by id;
store h into '/user/pig/full_text_t_map';

aa = load '/user/pig/full_text_t_map';       
bb = limit a 3;
dump bb;

-- load map type and extract tweeters who have tweeted word 'I' more than 5 times

a = load '/user/pig/full_text_t_map' as (id:chararray, freq_word:bag{t:(id1:chararray, freq_word_m:map[])});
b = foreach a generate id, flatten(freq_word) as (id1, freq_word_m);
c = filter b by (int)freq_word_m#'I' > 5;
d = limit c 10;
dump d;


-- 3.3 BAG example (star expression)

quit

[hdfs@sandbox ~]$ echo -e "user1\ta\tb\tc\nuser2\ta\tb\nuser3\ta" > data_test_bag
[hdfs@sandbox ~]$ cat data_test_bag
[hdfs@sandbox ~]$ hadoop fs -rmr /user/pig/data_test_bag
[hdfs@sandbox ~]$ hadoop fs -put data_test_bag /user/pig/data_test_bag



a = load '/user/pig/data_test_bag' using PigStorage('\t') as (id:chararray, f1:chararray, f2:chararray, f3:chararray);
b = group a ALL;
c = foreach b generate COUNT(a.$0);
d = foreach b generate COUNT(a.$1);
e = foreach b generate COUNT(a.$2);
f = foreach b generate COUNT(a.$3);
g = foreach b generate COUNT(a);
h = foreach b generate COUNT(a.*);   -- error
dump c;  -- 3
dump d;  -- 3
dump e;  -- 2
dump f;  -- 1
dump g;  -- 3



----------------------------------------------------------------------------
-- 4. Pig Functions
----------------------------------------------------------------------------

-------------------------
-- DATE/Time functions

-- CurrentTime()
-- GetYear()
-- GetMonth()
-- GetDay()
-- GetWeek()
-- ToDate()
-- ToString()
-- ToUnixTime()
-------------------------

-- 4.1 Date/Time functions

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, ToDate(ts) as ts1;
c = foreach b generate id, ts, ts1, ToString(ts1) as ts_iso, ToUnixTime(ts1), GetYear(ts1) as year, GetMonth(ts1) as month, GetWeek(ts1) as week;
d = limit c 5;
dump d;




--------------------------
-- STRING functions

-- LOWER()
-- UPPER()
-- STARTSWITH()
-- ENDSWITCH()
-- STRSPLIT()
-- STRSPLITTOBAG()
-- REPLACE()
-- SUBSTRING()
-- TRIM()
-- INDEXOF()
-- LASTINDEXOF()
-- REGEX_EXTRACT_ALL()
-- REGEX_EXTRACT()
--------------------------

-- 4.2 String Function - LOWER

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, LOWER(tweet) as tweet;
c = limit b 5;
dump c;

-- 4.3 String Function - UPPER

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, UPPER(tweet) as tweet;
c = limit b 5;
dump c;

-- 4.4 Use SUBSTRING() to extract year from ts string

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, SUBSTRING(ts, 0,4);
c = limit b 5;
dump c;

-- 4.5 Use STARTSWITH() to get retweets 

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = filter a by STARTSWITH(tweet,'RT');
c = group b all;
d = foreach c generate COUNT(b);
dump d;


-- 4.6 Find first twitter handles mentioned in a tweet using regex_extract() function

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, LOWER(tweet) as tweet;
c = foreach b generate id, ts, location, REGEX_EXTRACT(tweet, '(.*)@user_(\\S{8})([:| ])(.*)',2) as tweet;
d = limit c 5;
dump d;

-- 4.7 Find first 3 twitter handles mentioned in a tweet using regex_extract() function

-- method 1

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, LOWER(tweet) as tweet;
c = foreach b generate id, ts, location,
     REGEX_EXTRACT(tweet, '[^@]*@user_(\\S{8})[^@]*', 1) as mentions1,
     REGEX_EXTRACT(tweet, '[^@]*@user_(\\S{8})[^@]*@user_(\\S{8})[^@]*', 2) as mentions2,
     REGEX_EXTRACT(tweet, '[^@]*@user_(\\S{8})[^@]*@user_(\\S{8})[^@]*@user_(\\S{8})[^@]*', 3) as mentions3;
d = limit c 20;
dump d;

-- method 2

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, LOWER(tweet) as tweet;
c = foreach b generate id, ts, location,
     SUBSTRING(STRSPLIT(tweet,'@user_').$1,0,8), 
     SUBSTRING(STRSPLIT(tweet,'@user_').$2,0,8),
     SUBSTRING(STRSPLIT(tweet,'@user_').$3,0,8);
d = limit c 20;
dump d;

-- method 3

DEFINE TOP_ASC TOP('ASC'); 

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, LOWER(tweet) as tweet;
c = foreach b generate id, ts, location, tweet, FLATTEN(TOKENIZE(tweet)) as tokens;
d = foreach c generate id, ts, location, tweet, REGEX_EXTRACT(tokens,'.*@user_(\\w{8}).*',1) as token;
e = filter d by token IS NOT NULL;
f = foreach e generate id, ts, location, tweet, INDEXOF(tweet, token) as pos;
g = foreach f generate id, ts, pos, SUBSTRING(tweet,pos,pos+8) as mention;
h = group g by (id, ts);
i = foreach h {
        top3 = TOP_ASC(3,1,g);
        generate flatten(group) as (id, ts), top3 as mentions_top3;
    };
j = limit i 3000;
dump j;


-- 4.7 Finding users who tweet long tweets

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, SIZE(tweet) as tweet_len;
c = order b by tweet_len desc;
d = limit c 10;
dump d;

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, location, SIZE(REPLACE(tweet, '@USER_\\w{8}', '') ) as tweet_len;
c = order b by tweet_len desc;
d = limit c 10;
dump d;


-- 4.8 STRSPLIT(), STRSPLITTOBAG()

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, STRSPLITTOBAG(tweet, '[ ",()*]', 0);
c = limit b 3;
dump c;

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, FLATTEN(STRSPLITTOBAG(tweet, ' ', 0));
c = limit b 15;
dump c;


-- 4.9 Tweet word count using Pig TOKENIZE() and FLATTEN()

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate FLATTEN(TOKENIZE(tweet)) as token;
c = group b by token;
d = foreach c generate group as token, COUNT(b) as cnt;
e = order d by cnt desc;
f = limit e 20;
dump f;



--------------------------------------------------------------
-- CONDITIONAL function
--------------------------------------------------------------

-- 4.10 Find users who like to tw-eating

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, (GetHour(ToDate(ts))==7 ? 'breakfast' : 
                                   (GetHour(ToDate(ts))==12 ? 'lunch' :
                                       (GetHour(ToDate(ts))==19 ? 'dinner' : null))) as tw_eating, lat, lon;
c = filter b by tw_eating=='breakfast' or tw_eating=='lunch' or tw_eating=='dinner';
d = limit c 50;
dump d;





----------------------------------------------------------------------------
-- 5. Pig Relational Operations
----------------------------------------------------------------------------

-- 5.1 Find tweets that have mentions using FILTER

data = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
filtr = FILTER data BY tweet MATCHES '.*@USER_\\S{8}.*';
limt = limit filtr 500;
dump limt;


-- 5.2 Find all tweets by a user

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = filter a by id=='USER_ae406f1d'; 
dump b;


-- 5.3 Find all tweets tweeted from NYC vicinity (using bounding box -74.2589, 40.4774, -73.7004, 40.9176)
-- http://www.darrinward.com/lat-long/?id=461435


a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = filter a by lat > 40.4774 and lat < 40.9176 and lon > -74.2589 and lon < -73.7004 and 
                SIZE(tweet)<50 and 
                GetHour(ToDate(ts))==12;
c = foreach b generate lat, lon;
d = distinct c;
e = limit d 500;
dump e;


-- 5.4 Filtering data in pig
-- find retweets in NYC on 12th with length smaller than 50 characters

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = filter a by lat > 40.4774 and lat < 40.9176 and lon > -74.2589 and lon < -73.7004 and 
                SIZE(tweet)<50 and 
                GetHour(ToDate(ts))==12;
c = foreach b generate id, ts, lat, lon, tweet;
d = limit c 10;
dump d;

---------------------------
-- GROUP BY - Aggregation
---------------------------


-- 5.5 Calculate number of tweets per user 

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = group a by id;
c = foreach b generate group as id, COUNT(a) as cnt;
d = order c by cnt desc;
e = limit d 5;
dump e;

-- 5.6 Count total number of records

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = group a ALL;
c = foreach b generate COUNT_STAR(a);
dump c;


--------------
-- COGROUP ---
--------------

-- 5.7 COGROUP example

[hdfs@sandbox ~]$ echo -e "u1,14,M,US\nu2,32,F,UK\nu3,22,M,US" > /home/lab/user.txt
[hdfs@sandbox ~]$ hadoop fs -put /home/lab/user.txt /user/pig/user.txt

-- u1, 14, M, US
-- u2, 32, F, UK
-- u3, 22, M, US

[hdfs@sandbox ~]$ echo -e "u1,US\nu1,UK\nu1,CA\nu2,US" > /home/lab/session.txt
[hdfs@sandbox ~]$ hadoop fs -put /home/lab/session.txt /user/pig/session.txt

-- u1, US
-- u1, UK
-- u1, CA
-- u2, US

user = load '/user/pig/user.txt' using PigStorage(',') as (uid:chararray, age:int, gender:chararray, region:chararray);
session = load '/user/pig/session.txt' using PigStorage(',') as (uid:chararray, region:chararray);
C = cogroup user by uid, session by uid;
D = foreach C {
    crossed = cross user, session;
    generate crossed;
}
dump D;  

-- 5.8 Use COGROUP for SET Intersection 

[hdfs@sandbox ~]$ echo -e "John,3\nHarry,4\nGeorge,2" > /home/lab/s1.txt
[hdfs@sandbox ~]$ echo -e "John,2\nJohn,3\nGeorge,0\nSue,1" > /home/lab/s2.txt
[hdfs@sandbox ~]$ hadoop fs -put s1.txt /user/pig/  
[hdfs@sandbox ~]$ hadoop fs -put s2.txt /user/pig/  

s1 = load '/user/pig/s1.txt' using PigStorage(',') as (name:chararray, hits:int);
s2 = load '/user/pig/s2.txt' using PigStorage(',') as (name:chararray, errors:int);
grps = COGROUP s1 BY name, s2 BY name;
grps2 = FILTER grps by NOT(IsEmpty(s1)) AND NOT(IsEmpty(s2));
dump grps2;

-- 5.9 Use COGROUP for set difference 

s1 = load '/user/pig/s1.txt' using PigStorage(',') as (name:chararray, hits:int);
s2 = load '/user/pig/s2.txt' using PigStorage(',') as (name:chararray, errors:int);
grps = COGROUP s1 BY name, s2 BY name;
grps2 = FILTER grps by IsEmpty(s2);
set_diff = FOREACH grps2 GENERATE group as grp, s1, s2 ;
dump set_diff;


--------------------------------------
-- ORDER BY
--------------------------------------

-- 5.10 Find top 10 tweeters in NYC

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = filter a by lat > 40.4774 and lat < 40.9176 and
      lon > -74.2589 and lon < -73.7004;
c = group b by id;
d = foreach c generate group as id, COUNT(b) as cnt;
e = order d by cnt desc;
f = limit e 10;
dump f;


--------------------------------------
-- Advanced JOIN
--------------------------------------


-- prepare lookup table 'dayofweek'

fs -put /home/lab/dayofweek.txt /user/pig/


-- 5.11 Find Weekend Tweets
-- INNER JOIN

a = load '/user/pig/full_text.txt' using PigStorage('\t') AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
a1 = foreach a generate id, ts, SUBSTRING(ts,0,10) as date;

b = load '/user/pig/dayofweek.txt' using PigStorage('\t') as (date:chararray, dow:chararray);
b1 = filter b by dow=='Saturday' or dow=='Sunday';

c = join a1 by date, b1 by date;
d = foreach c generate a1::id .. a1::date, b1::dow as dow;
e = limit d 5;
dump e;


-- 5.12 Find Weekend Tweets
-- Using Replicated JOIN

a = load '/user/pig/full_text.txt' using PigStorage('\t') AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
a1 = foreach a generate id, ts, SUBSTRING(ts,0,10) as date;

b = load '/user/pig/dayofweek.txt' using PigStorage('\t') as (date:chararray, dow:chararray);
b1 = filter b by dow=='Saturday' or dow=='Sunday';

c = join a1 by date, b1 by date using 'replicated';
d = foreach c generate a1::id .. a1::date, b1::dow as dow;
e = limit d 5;
dump e;


---------------------
-- Flatten
---------------------

-- 5.13 Flatten Tuples
-- Calculate number of tweets per user per day

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, SUBSTRING(ts, 0, 10) as date, lat, lon, tweet;
c = GROUP b BY (id, date);
d = FOREACH c GENERATE FLATTEN(group) AS (id,date) , COUNT(b) as cnt;
e = order d by cnt desc;
f = limit e 5;
dump f;

-- visualize group
illustrate d;



-- 5.14 Flatten Bags
-- Flatten Bags Example

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, SUBSTRING(ts, 0, 10) as date, lat, lon, tweet;
c = GROUP b BY (id, date);
d = FOREACH c GENERATE FLATTEN(b) AS (id, date, lat, lon, tweet);
e = order d by id, date;
f = limit e 50;
dump f;



--------------------------------------
-- Nested Foreach
--------------------------------------


-- 5.15 Get top word of each user and store in bags

-- method 1

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, TOTUPLE(lat, lon) as loc_tuple:tuple(lat:chararray, lon:chararray), flatten(TOKENIZE(tweet)) as token;
c = group b by (id, token);
d = foreach c generate flatten(group) as (id, token), COUNT(b) as cnt; 
e = group d by id;
f = foreach e {
	sortd = order d by cnt desc;
	top = limit sortd 10;
	generate group as id, top as pop_word_bag;
};
g = limit f 10;
dump g;


-- method 2

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, TOTUPLE(lat, lon) as loc_tuple:tuple(lat:chararray, lon:chararray), flatten(TOKENIZE(tweet)) as token;
c = group b by (id, token);
d = foreach c generate flatten(group) as (id, token), COUNT(b) as cnt; 
e = group d by id;
f = foreach e generate group as id, TOP(10, 2, d);
g = limit f 10;
dump g;



-- 5.16 Nested Foreach example

[hdfs@sandbox ~]$ echo -e "www.ccc.com,www.hjk.com\nwww.ddd.com,www.xyz.org\nwww.aaa.com,www.cvn.org\nwww.www.com,www.kpt.net\nwww.www.com,www.xyz.org\nwww.ddd.com,www.xyz.org" > /home/lab/url.txt
[hdfs@sandbox ~]$ hadoop fs -put url.txt /user/pig/

A = LOAD '/user/pig/url.txt' using PigStorage(',') AS (url:chararray,outlink:chararray);
B = GROUP A BY url;
X = FOREACH B {
        FA= FILTER A BY outlink == 'www.xyz.org';
        PA = FA.outlink;
        GENERATE group, COUNT(PA);
}
dump X;	


-------------------
-- CROSS
-------------------

-- 5.17 Parameter distribution using CROSS
-- parameter file
(nfriends, 2)
(ndays, 100)
(nvists, 13)

-- friend table
(Amy, George)
(George, Fred)
(Fred, Anne)
(George, Joe)
(George, Harry)


[hdfs@sandbox ~]$ echo -e "nfriends,2\nndays,100\nnvisits,13" > /home/lab/params.txt
[hdfs@sandbox ~]$ echo -e "Amy,George\nGeorge,Fred\nFred,Anne\nGeorge,Joe\nGeorge,Harry" > /home/lab/friend.txt
[hdfs@sandbox ~]$ hadoop fs -put params.txt /user/pig/  
[hdfs@sandbox ~]$ hadoop fs -put friend.txt /user/pig/  

params = load '/user/pig/params.txt' using PigStorage(',') as (p_name:chararray, value:int);
friend = load '/user/pig/friend.txt' using PigStorage(',') as (name:chararray, friend:chararray);

friend_grp = group friend by name;
friend_cnt = foreach friend_grp generate group as name, COUNT(friend.friend) as cnt;

friend_param = filter params by p_name=='nfriends';
friend_param_p = foreach friend_param generate value;

friend_cross = CROSS friend_cnt, friend_param_p;
friend_cross_1 = filter friend_cross by friend_cnt::cnt >= friend_param_p::value;
dump friend_cross_1;

--------------------------------------
-- Scalar Projection
--------------------------------------

-- 5.18 Normalize the number of tweets of each user against global average number

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = group a by id;
c = foreach b generate group as id, COUNT(a) as user_cnt;
d = group c ALL;
e = foreach d generate AVG(c.user_cnt) as global_avg;
f = foreach c generate id, user_cnt/(float)e.global_avg as index;
store f into '/user/pig/tweet_count_index';

a = load '/user/pig/tweet_count_index/*' as (id:chararray, index:float);
b = limit a 10;
dump b;



----------------------------------------------------------------------------
-- 6. Pig UDF (User Defined Function)
----------------------------------------------------------------------------

-----------------
-- piggybank UDFs
-----------------

-- 6.1 iso time to unix time conversion
-- register UDFs and define functions first

register '/home/lab/piggybank-0.15.0.jar';
define isotounix org.apache.pig.piggybank.evaluation.datetime.convert.ISOToUnix();

a = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
b = foreach a generate id, ts, isotounix(ts) as ts_unix;
c = limit b 3;
dump c;

-----------------
-- DataFu UDFs
-----------------

-- 6.2 Calculate median latitude value using DataFu median function

-- register UDFs and define functions first

register /home/lab/datafu-pig-incubating-1.3.0.jar
define Median datafu.pig.stats.StreamingMedian();
data = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
data1 = foreach (group data ALL) generate Median(data.lat);
dump data1;

-- 6.3 Simple Random Sampling
-- Take a 1% random sample from the dataset

-- register UDFs and define functions first

register /home/lab/datafu-pig-incubating-1.3.0.jar
DEFINE SRS datafu.pig.sampling.SimpleRandomSample('0.01');

data = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
sampled = foreach (group data all) generate flatten(SRS(data));
store sampled into '/user/pig/full_text_src';


-- 6.4 Stratified Sampling (by date)
-- Take a 1% random sample from each date using SRS and group by date

-- register UDFs and define functions first

register /home/lab/datafu-pig-incubating-1.3.0.jar
DEFINE SRS datafu.pig.sampling.SimpleRandomSample('0.01');

data = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:float, lon:float, tweet:chararray);
data1 = foreach data generate id, SUBSTRING(ts, 0, 10) as date, lat, lon, tweet;
grouped = group data1 BY date;
sampled = foreach grouped generate flatten(SRS(data1));
store sampled into '/user/pig/full_text_stratified';


-----------------
-- Pigeon UDFs
-----------------

-- 6.5 Find tweets tweeted from NYC using bounding box and Pigeon UDF
-- Plot the results at: http://www.darrinward.com/lat-long/?id=490564 

-- register UDFs 
register /home/lab/pigeon-0.1.jar;
register /home/lab/esri-geometry-api-1.2.1.jar;

-- define functions
DEFINE ST_MakeBox edu.umn.cs.pigeon.MakeBox;
DEFINE ST_Contains edu.umn.cs.pigeon.Contains;
DEFINE ST_MakePoint edu.umn.cs.pigeon.MakePoint;


data = load '/user/pig/full_text.txt' AS (id:chararray, ts:chararray, location:chararray, lat:double, lon:double, tweet:chararray);
data1 = FOREACH data GENERATE id, ts, lat, lon, ST_MakePoint(lat, lon) AS geom_point, tweet;
data2 = FILTER data1 BY ST_Contains(ST_MakeBox(40.4774, -74.2589, 40.9176, -73.7004), geom_point);
data3 = limit data2 200;
data4 = foreach data3 generate lat, lon;
dump data4;
